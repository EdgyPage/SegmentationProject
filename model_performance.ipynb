{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import zipfile\n",
    "import os\n",
    "import decimal as d\n",
    "import numpy as np\n",
    "import json\n",
    "import unet\n",
    "import image_processor\n",
    "import torchvision\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import itertools\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.linear_model import LogisticRegression, LinearRegression\n",
    "import pickle\n",
    "import fnmatch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def modelLoader(model, path: str):\n",
    "    return model.load_state_dict(torch.load(path))\n",
    "\n",
    "#thanks, Chat GPT\n",
    "def unzip_file(zip_file_path, extract_to_path):\n",
    "    with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n",
    "        zip_ref.extractall(extract_to_path)\n",
    "\n",
    "#thanks, Chat GPT\n",
    "def sortFirstNFiles(directoryPath: str, n: int = 10):\n",
    "    allFiles = [os.path.join(directoryPath, file) for file in os.listdir(directoryPath) if file.endswith(('pt', '.pt'))]\n",
    "    #sorts filenames based on third segment (validationloss in name)\n",
    "    sortedFiles = sorted(allFiles, key = lambda fileName: int(fileName.split(os.sep)[-1].split('_')[2]))\n",
    "\n",
    "    firstFiles = sortedFiles[:n]\n",
    "\n",
    "    return firstFiles\n",
    "\n",
    "def mapImageTensor(tensor: torch.Tensor, center = 0, scale = 1):\n",
    "    return 1/ (1 + torch.exp(-(tensor - center)/scale))\n",
    "\n",
    "def highPrecisionArray(numOfSamples, beg=0, last=1, precision = 4):\n",
    "    d.getcontext().prec = precision\n",
    "    start = d.Decimal(beg)\n",
    "    end = d.Decimal(last)\n",
    "    step = (end - start) / d.Decimal(numOfSamples - 1)\n",
    "    numArray = [start + i * step for i in range(numOfSamples)]\n",
    "    numArray = [float(x) for x in numArray]\n",
    "    return numArray\n",
    "\n",
    "def metrics(pred, label):\n",
    "    if torch.is_tensor(pred) or torch.is_tensor(label):\n",
    "        pred = pred.detach().cpu().numpy()\n",
    "        label = label.detach().cpu().numpy()\n",
    "    \n",
    "    TP = ((label == 255) & (pred == 255)).sum()\n",
    "    FP = ((label == 0) & (pred == 255)).sum()\n",
    "    TN = ((label == 0) & (pred == 0)).sum()\n",
    "    FN = ((label == 255) & (pred == 0)).sum()\n",
    "\n",
    "    return int(TP), int(FP), int(TN), int(FN)\n",
    "\n",
    "def dice(TP, FP, FN):\n",
    "    if (2*TP + FP + FN) == 0:\n",
    "        return 0\n",
    "    return round(2*TP / ( 2*TP + FP + FN), 3)\n",
    "\n",
    "#thanks, Chat GPT\n",
    "def rectTriArea(x_values, y_values):\n",
    "    n = len(x_values)\n",
    "    area = 0.0\n",
    "    for i in range(1, n):\n",
    "        width = x_values[i] - x_values[i - 1]\n",
    "        height = min(y_values[i - 1], y_values[i])\n",
    "        rectArea = width*height\n",
    "\n",
    "        deltaHeight = max(y_values[i], y_values[i-1]) - height\n",
    "\n",
    "        triArea = .5 * deltaHeight*width\n",
    "        area += rectArea + triArea\n",
    "    return area\n",
    "\n",
    "def framePlotter(pred , label, frameNum: int, caseNum: int, modelName: str, saveDir: str):\n",
    "    if torch.is_tensor(pred) or torch.is_tensor(label):\n",
    "        pred = pred.detach().cpu().numpy()\n",
    "        label = label.detach().cpu().numpy()\n",
    "    plt.subplot(2,1,1)\n",
    "    plt.imshow(pred.transpose(1,2,0))\n",
    "    #parts = modelName.split(\"\\\\\")\n",
    "    #modelName = parts[3]\n",
    "    plt.title(f'Pred- {modelName} | C: {caseNum} F: {frameNum}')\n",
    "\n",
    "    plt.subplot(2,1,2)\n",
    "    plt.imshow(label.transpose(1,2,0))\n",
    "    plt.title(f'Label- {modelName} | C: {caseNum} F: {frameNum}')\n",
    "    savePath = os.path.join(saveDir, modelName + '_C_' + str(caseNum) + '_F_' + str(frameNum) + '.png')\n",
    "    plt.savefig(savePath)\n",
    "    plt.show()\n",
    "\n",
    "def findLogModels(logModelDir, modelName):\n",
    "    matches = []\n",
    "    for root, dirnames, filenames in os.walk(logModelDir):\n",
    "        for filename in filenames:\n",
    "            if fnmatch.fnmatch(filename, f'*{modelName}'):\n",
    "                matches.append(os.path.join(root, filename))\n",
    "    return matches\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = 'cuda'\n",
    "else:\n",
    "    device = 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "START: C:\\Users\\myfir\\My Drive\\Segmentation_Files\\Models\\Test\\unet\\unet16x_VL_98_CE_144_TE_400_LR_0.001_BS_16_TS_2024-04-12_22-46-01.pt | 2024-05-09 02:28:35\n",
      "Starting Log Model Loader\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "y should be a 1d array, got an array of shape (880, 1024) instead.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 90\u001b[0m\n\u001b[0;32m     87\u001b[0m transformedArrayOfFlattenedOutputs \u001b[38;5;241m=\u001b[39m poly\u001b[38;5;241m.\u001b[39mfit_transform(arrayOfFlattenedOutputs)\n\u001b[0;32m     88\u001b[0m arrayOfLabels \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(listOfLabels)\n\u001b[1;32m---> 90\u001b[0m logModel \u001b[38;5;241m=\u001b[39m \u001b[43mLogisticRegression\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtransformedArrayOfFlattenedOutputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43marrayOfLabels\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     92\u001b[0m probabilities \u001b[38;5;241m=\u001b[39m logModel\u001b[38;5;241m.\u001b[39mpredict_proba(transformedArrayOfFlattenedOutputs)\n\u001b[0;32m     94\u001b[0m bestScore \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\myfir\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\base.py:1151\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1144\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1146\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1147\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1148\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1149\u001b[0m     )\n\u001b[0;32m   1150\u001b[0m ):\n\u001b[1;32m-> 1151\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\myfir\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1207\u001b[0m, in \u001b[0;36mLogisticRegression.fit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m   1204\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1205\u001b[0m     _dtype \u001b[38;5;241m=\u001b[39m [np\u001b[38;5;241m.\u001b[39mfloat64, np\u001b[38;5;241m.\u001b[39mfloat32]\n\u001b[1;32m-> 1207\u001b[0m X, y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_data\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1208\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1209\u001b[0m \u001b[43m    \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1210\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccept_sparse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcsr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1211\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1212\u001b[0m \u001b[43m    \u001b[49m\u001b[43morder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mC\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1213\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccept_large_sparse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msolver\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mliblinear\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msag\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msaga\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1214\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1215\u001b[0m check_classification_targets(y)\n\u001b[0;32m   1216\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclasses_ \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39munique(y)\n",
      "File \u001b[1;32mc:\\Users\\myfir\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\base.py:621\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[1;34m(self, X, y, reset, validate_separately, cast_to_ndarray, **check_params)\u001b[0m\n\u001b[0;32m    619\u001b[0m         y \u001b[38;5;241m=\u001b[39m check_array(y, input_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcheck_y_params)\n\u001b[0;32m    620\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 621\u001b[0m         X, y \u001b[38;5;241m=\u001b[39m \u001b[43mcheck_X_y\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mcheck_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    622\u001b[0m     out \u001b[38;5;241m=\u001b[39m X, y\n\u001b[0;32m    624\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m check_params\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mensure_2d\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mTrue\u001b[39;00m):\n",
      "File \u001b[1;32mc:\\Users\\myfir\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\utils\\validation.py:1163\u001b[0m, in \u001b[0;36mcheck_X_y\u001b[1;34m(X, y, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, estimator)\u001b[0m\n\u001b[0;32m   1143\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1144\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mestimator_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m requires y to be passed, but the target y is None\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1145\u001b[0m     )\n\u001b[0;32m   1147\u001b[0m X \u001b[38;5;241m=\u001b[39m check_array(\n\u001b[0;32m   1148\u001b[0m     X,\n\u001b[0;32m   1149\u001b[0m     accept_sparse\u001b[38;5;241m=\u001b[39maccept_sparse,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1160\u001b[0m     input_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mX\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   1161\u001b[0m )\n\u001b[1;32m-> 1163\u001b[0m y \u001b[38;5;241m=\u001b[39m \u001b[43m_check_y\u001b[49m\u001b[43m(\u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmulti_output\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmulti_output\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_numeric\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43my_numeric\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mestimator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mestimator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1165\u001b[0m check_consistent_length(X, y)\n\u001b[0;32m   1167\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m X, y\n",
      "File \u001b[1;32mc:\\Users\\myfir\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\utils\\validation.py:1184\u001b[0m, in \u001b[0;36m_check_y\u001b[1;34m(y, multi_output, y_numeric, estimator)\u001b[0m\n\u001b[0;32m   1182\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1183\u001b[0m     estimator_name \u001b[38;5;241m=\u001b[39m _check_estimator_name(estimator)\n\u001b[1;32m-> 1184\u001b[0m     y \u001b[38;5;241m=\u001b[39m \u001b[43mcolumn_or_1d\u001b[49m\u001b[43m(\u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwarn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m   1185\u001b[0m     _assert_all_finite(y, input_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my\u001b[39m\u001b[38;5;124m\"\u001b[39m, estimator_name\u001b[38;5;241m=\u001b[39mestimator_name)\n\u001b[0;32m   1186\u001b[0m     _ensure_no_complex_data(y)\n",
      "File \u001b[1;32mc:\\Users\\myfir\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\utils\\validation.py:1245\u001b[0m, in \u001b[0;36mcolumn_or_1d\u001b[1;34m(y, dtype, warn)\u001b[0m\n\u001b[0;32m   1234\u001b[0m         warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m   1235\u001b[0m             (\n\u001b[0;32m   1236\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mA column-vector y was passed when a 1d array was\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1241\u001b[0m             stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m,\n\u001b[0;32m   1242\u001b[0m         )\n\u001b[0;32m   1243\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _asarray_with_order(xp\u001b[38;5;241m.\u001b[39mreshape(y, (\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m,)), order\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mC\u001b[39m\u001b[38;5;124m\"\u001b[39m, xp\u001b[38;5;241m=\u001b[39mxp)\n\u001b[1;32m-> 1245\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1246\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my should be a 1d array, got an array of shape \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m instead.\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(shape)\n\u001b[0;32m   1247\u001b[0m )\n",
      "\u001b[1;31mValueError\u001b[0m: y should be a 1d array, got an array of shape (880, 1024) instead."
     ]
    }
   ],
   "source": [
    "\n",
    "\"\"\"\n",
    "sigmoid val tensors\n",
    "flatten val tensors\n",
    "add flattened val tensors to array\n",
    "flatten and add to list labels\n",
    "polytransform array to Z dimension\n",
    "logistic regresson on Z, list labels\n",
    "apply model to predict result of sig-flat test images\n",
    "construct prediction label from output list\n",
    "compare pred label to label\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "size of Z dim = (n+(d-1))Cd \n",
    "(n+(d-1)) Choose d degrees\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "Take val data\n",
    "Fit curve to val data\n",
    "find threshold through search\n",
    "take fit curve and threshold and apply to test set\n",
    "compare\n",
    "\"\"\"\n",
    "\n",
    "degree = 2\n",
    "downscale = 16\n",
    "testCases = 4\n",
    "imagesPerTestCase = 100\n",
    "numOfModelsToTest = 1\n",
    "height = int(256/downscale)\n",
    "width = int(1024/downscale)\n",
    "shape = (height, width)\n",
    "\n",
    "logModelDir = r\"C:\\Users\\myfir\\My Drive\\Segmentation_Files\\Performance_Results\\5-9\\LogModelAndGraphs\"\n",
    "modelDirectory = r\"C:\\Users\\myfir\\My Drive\\Segmentation_Files\\Models\\Test\\unet\"\n",
    "\n",
    "thresholds = highPrecisionArray(31, 0, 1, 5)\n",
    "\n",
    "transform = torchvision.transforms.Compose([\n",
    "    torchvision.transforms.Resize((height, width))])\n",
    "\n",
    "\n",
    "logModelImagePath = r\"C:\\Users\\myfir\\My Drive\\Segmentation_Files\\Datasets\\Rename_Data\\train_val\\Rename_TrainVal_Image\"\n",
    "logModelLabelPath = r\"C:\\Users\\myfir\\My Drive\\Segmentation_Files\\Datasets\\Rename_Data\\train_val\\Rename_TrainVal_Label\"\n",
    "segmentationPath = f\"{logModelDir}\\Segmentations\"\n",
    "\n",
    "if not os.path.exists(segmentationPath):\n",
    "    os.makedirs(segmentationPath)\n",
    "\n",
    "testImagePath = r\"C:\\Users\\myfir\\My Drive\\Segmentation_Files\\Datasets\\Rename_Data\\test\\Rename_Test_Image\"\n",
    "testLabelPath = r\"C:\\Users\\myfir\\My Drive\\Segmentation_Files\\Datasets\\Rename_Data\\test\\Rename_Test_Label\"\n",
    "\n",
    "topModelPaths = sortFirstNFiles(modelDirectory, numOfModelsToTest)\n",
    "\n",
    "logModelFitLoader= image_processor.imageDirsToTestLoader(imageDir=logModelImagePath, labelDir= logModelLabelPath, transform=transform)\n",
    "testLoader = image_processor.imageDirsToTestLoader(imageDir=testImagePath, labelDir=testLabelPath, transform=transform)\n",
    "\n",
    "for modelPath in topModelPaths:\n",
    "    currentTime = datetime.now()\n",
    "    formatTime = currentTime.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "    print(f'START: {modelPath} | {formatTime}')\n",
    "    print(f'Starting Log Model Loader')\n",
    "    model = unet.unet()\n",
    "    model.loadAttributes(modelPath)\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    listOfFlattenedOutputs = []\n",
    "    listOfLabels = []\n",
    "\n",
    "    for i, (image, label) in enumerate(logModelFitLoader):\n",
    "        #with batch size of 1, i ranges from 0-399 in test set of 400 images\n",
    "        #0-879 for training set\n",
    "        image = image.to(device)\n",
    "        label = label.to(device)\n",
    "        output = model(image)\n",
    "        #output = mapImageTensor(output)\n",
    "        outputArray = output.detach().cpu().numpy().reshape(-1)\n",
    "        labelArray = label.detach().cpu().numpy().reshape(-1)\n",
    "\n",
    "        listOfFlattenedOutputs.append(outputArray)\n",
    "        listOfLabels.append(labelArray)\n",
    "    \n",
    "    arrayOfFlattenedOutputs = np.array(listOfFlattenedOutputs)\n",
    "    poly = PolynomialFeatures(degree = degree)\n",
    "    #880, 525825\n",
    "    transformedArrayOfFlattenedOutputs = poly.fit_transform(arrayOfFlattenedOutputs)\n",
    "    #880, 1024\n",
    "    arrayOfLabels = np.array(listOfLabels)\n",
    "\n",
    "    logModel = LogisticRegression().fit(transformedArrayOfFlattenedOutputs, arrayOfLabels)\n",
    "\n",
    "    probabilities = logModel.predict_proba(transformedArrayOfFlattenedOutputs)\n",
    "\n",
    "    bestScore = 0\n",
    "    bestThreshold = 0\n",
    "\n",
    "    currentTime = datetime.now()\n",
    "    formatTime = currentTime.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "    print(f'START: Threshold Search | {formatTime}')\n",
    "\n",
    "    for i, t in enumerate(thresholds):\n",
    "        currentTime = datetime.now()\n",
    "        formatTime = currentTime.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "        print(f'START: Threshold: {t} | {formatTime}')\n",
    "        yPred = (probabilities[:, 1] > t).astype(int)\n",
    "        yPredArray = np.array(yPred)\n",
    "        yPredArray[yPredArray > .9] = 255\n",
    "        TP, FP, TN, FN = metrics(yPredArray, arrayOfLabels)\n",
    "        score = dice(TP, FP, FN)\n",
    "        if score > bestScore:\n",
    "            bestScore = score\n",
    "            bestThreshold = t\n",
    "    \n",
    "    logModelName = f'logModel_{model.name}_degree_{degree}_dice_{bestScore}_t_{bestThreshold}'\n",
    "    logModelPath = os.path.join(logModelDir, f'{logModelName}.pkl')\n",
    "\n",
    "    with open(logModelPath, 'wb') as file:\n",
    "        pickle.dump(logModel, file)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "for modelPath in topModelPaths:\n",
    "    currentTime = datetime.now()\n",
    "    formatTime = currentTime.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "    print(f'START: {modelPath} | {formatTime}')\n",
    "    print(f'Starting Log Model Loader')\n",
    "    model = unet.unet()\n",
    "    model.loadAttributes(modelPath)\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    \n",
    "    poly = PolynomialFeatures(degree = degree)\n",
    "    matches = findLogModels(logModelDir, model.name)\n",
    "\n",
    "    for match in matches:\n",
    "        print(f'Analyzing {model.name} on Log Model {match}')\n",
    "\n",
    "        logModel = pickle.load(match)\n",
    "        parts = match.split('_')\n",
    "        parts = parts[-1].split('.pkl')\n",
    "        t = float(parts[0])\n",
    "        diceByFrame = [0]*100\n",
    "        caseNum = 0\n",
    "        \n",
    "        #done with a batch size of one to not stress equipment\n",
    "        for i, (image, label) in enumerate(testLoader):\n",
    "            #with batch size of 1, i ranges from 0-399 in test set of 400 images\n",
    "            #0-779 for training set\n",
    "            image = image.to(device)\n",
    "            label = label.to(device)\n",
    "            output = model(image)\n",
    "            #output = mapImageTensor(output)\n",
    "            outputArray = output.detach().cpu().numpy().reshape(-1)\n",
    "            labelArray = label.detach().cpu().numpy().reshape(-1)\n",
    "\n",
    "            transformedArrayOfFlattenedOutputs = poly.fit_transform(outputArray)\n",
    "            probabilities = logModel.predict_proba(transformedArrayOfFlattenedOutputs)\n",
    "            yPred = (probabilities[:, 1] > t).astype(int)\n",
    "            yPredArray = np.array(yPred)\n",
    "\n",
    "\n",
    "            TP, FP, TN, FN = metrics(yPredArray, labelArray)\n",
    "            score = dice(TP, FP, FN)\n",
    "\n",
    "            #reshaping flattened array to graph predictions\n",
    "            yPredArray.reshape(shape)\n",
    "            labelArray.reshape(shape)\n",
    "\n",
    "            frameNum = i%100\n",
    "            if frameNum == 0:\n",
    "                caseNum += 1\n",
    "\n",
    "            diceByFrame[frameNum] += score\n",
    "            framePlotter(pred=yPredArray, label=labelArray, frameNum=frameNum, caseNum=caseNum, modelName= match, saveDir=segmentationPath)\n",
    "\n",
    "            \n",
    "\n",
    "        diceByFrame = list(map(lambda x: x * .25, diceByFrame))\n",
    "\n",
    "        plt.plot(diceByFrame)\n",
    "        plt.title(f\"Avg. Dice by Frame for {match}\")\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelDirectory = r\"\"\n",
    "thresholds = highPrecisionArray(20, 0, 1, 5)\n",
    "scales = highPrecisionArray(20, 1 , 500)\n",
    "centers = highPrecisionArray(25, 0, 255)\n",
    "scaleCentersProduct = list(itertools.product(scales, centers))\n",
    "\n",
    "#downscale = torchvision.transforms.Compose([\n",
    "#    torchvision.transforms.Resize((16, 64))])\n",
    "\n",
    "testImagePath = r\"\"\n",
    "testLabelPath = r\"\"\n",
    "\n",
    "topModelPaths = sortFirstNFiles(modelDirectory, 1)\n",
    "logModelFitLoader= image_processor.imageDirsToTestLoader(imageDir=testImagePath, labelDir= testLabelPath)\n",
    "\n",
    "allDiceResults = {path : {} for path in topModelPaths}\n",
    "allMetricResults = {path : {} for path in topModelPaths}\n",
    "#allFrameResults = {path : {} for path in topModelPaths}\n",
    "\n",
    "for modelPath in topModelPaths:\n",
    "    currentTime = datetime.now()\n",
    "    formatTime = currentTime.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "    print(f'START: {modelPath} | {formatTime}')\n",
    "    model = unet.unet()\n",
    "    model.load_state_dict(torch.load(modelPath))\n",
    "    #model.to(device)\n",
    "    model.eval()\n",
    "    \n",
    "\n",
    "    diceResultsByScaleCenter = {f'{s}, {c}': {} for s, c in scaleCentersProduct}\n",
    "    metricResultsByScaleCenter = {f'{s}, {c}': {} for s, c in scaleCentersProduct}\n",
    "    #predLabelResultsByScaleCenter = {f'{s}, {c}': {} for s, c in scaleCentersProduct}\n",
    "\n",
    "    for s, c in scaleCentersProduct:\n",
    "        currentTime = datetime.now()\n",
    "        formatTime = currentTime.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "        print(f'SCALE: {s} - CENTER: {c} | {formatTime}')\n",
    "\n",
    "        diceByThreshold = {str(t): [0]*100 for t in thresholds}\n",
    "        metricsByThreshold = {str(t): [[0, 0, 0, 0]]*100 for t in thresholds}\n",
    "        #caseByThreshold = {str(t): {str(i+1): [] for i in range(len(testLoader))} for t in thresholds}\n",
    "\n",
    "        for i, (image, label) in enumerate(logModelFitLoader): #batch of 100\n",
    "            #image = image.to(device)\n",
    "            #label = label.to(device)\n",
    "            pred = model(image)\n",
    "\n",
    "            predSig = mapImageTensor(pred, center= c, scale= s)\n",
    "\n",
    "            for j in range(predSig.size(0)): #split each batch to single frames\n",
    "                frameImage = predSig[j, :, :, :]\n",
    "                frameLabel = label[j, :, :, :]\n",
    "\n",
    "                for t in thresholds:\n",
    "                        predThresh = torch.where(frameImage <= t, 0, 255)\n",
    "                        \n",
    "                        \n",
    "                        TP, FP, TN, FN = metrics(predThresh, frameLabel)\n",
    "\n",
    "                        diceCoeffPerFrame = dice(TP, FP, FN)\n",
    "\n",
    "                        diceByThreshold[str(t)][j] += diceCoeffPerFrame\n",
    "\n",
    "                        for k, metric in enumerate([TP, FP, TN, FN]):\n",
    "                             metricsByThreshold[str(t)][j][k] += metric\n",
    "\n",
    "\n",
    "                        #caseByThreshold[str(t)][str(i+1)].append([predThresh.cpu().detach(), frameLabel.cpu().detach()])\n",
    "\n",
    "                        #oldTP, oldFP, oldTN, oldFN = metricsByThreshold[str(t)][j]\n",
    "                        #newTP = oldTP + TP\n",
    "                        #newFP = oldFP + FP\n",
    "                        #newTN = oldTN + TN\n",
    "                        #newFN = oldFN + FN\n",
    "                        #metricsByThreshold[str(t)][j] = (newTP, newFP, newTN, newFN)\n",
    "\n",
    "                            \n",
    "\n",
    "        for t in thresholds:\n",
    "            diceByThreshold[str(t)] = [val * .25 for val in diceByThreshold[str(t)]]\n",
    "            #metricsByThreshold[str(t)] = [[val * .25 for val in metricByFrame] for metricByFrame in metricsByThreshold[str(t)]]\n",
    "\n",
    "                      \n",
    "        metricResultsByScaleCenter[f'{s}, {c}'] = metricsByThreshold\n",
    "        diceResultsByScaleCenter[f'{s}, {c}'] = diceByThreshold\n",
    "        #predLabelResultsByScaleCenter[f'{s}, {c}'] = caseByThreshold\n",
    "\n",
    "    allDiceResults[modelPath] = diceResultsByScaleCenter\n",
    "    allMetricResults[modelPath] = metricResultsByScaleCenter\n",
    "    #allFrameResults[modelPath] = predLabelResultsByScaleCenter\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "currentTime = datetime.now()\n",
    "formatTime = currentTime.strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "parentResultsPath = r\"\"\n",
    "diceSavePath = f'{parentResultsPath}\\\\allDiceResults_{formatTime}.txt'\n",
    "metricSavePath = f'{parentResultsPath}\\\\allMetricResults_{formatTime}.txt'\n",
    "#frameSavePath = f'{parentResultsPath}\\\\allFrameResults_{formatTime}.txt'\n",
    "\n",
    "with open(diceSavePath, 'w') as saveFile:\n",
    "    saveFile.write(json.dumps(allDiceResults))\n",
    "\n",
    "\"\"\"\n",
    "edit = {path : {} for path in topModelPaths}\n",
    "\n",
    "for modelPath in topModelPaths:\n",
    "    edit1 = {f'{s}, {c}': {} for s, c in scaleCentersProduct}\n",
    "    for s,c in scaleCentersProduct:\n",
    "        edit2 = {str(t): []*100 for t in thresholds}\n",
    "        for t in thresholds:\n",
    "            listOfLists = allMetricResults[modelPath][f'{s}, {c}'][f'{t}']\n",
    "            for l in listOfLists:\n",
    "                change = list(map(int, l))\n",
    "                edit2[f'{t}'].append(change)\n",
    "        edit1[f'{s}, {c}'] = edit2\n",
    "    edit[modelPath] = edit1\n",
    "\"\"\"\n",
    "\n",
    "    \n",
    "with open(metricSavePath, 'w') as saveFile:\n",
    "    saveFile.write(json.dumps(allMetricResults))\n",
    "\n",
    "#torch.save(allFrameResults, frameSavePath)\n",
    "\n",
    "\n",
    "#topAreas = [(0,0,0,0)]* len(scales) * len(thresholds)\n",
    "topAreas = []\n",
    "frameData = [n for n in range(1, 101)]\n",
    "\n",
    "for modelPath in topModelPaths:\n",
    "    for s, c in scaleCentersProduct:\n",
    "        for t in thresholds:\n",
    "            diceList = allDiceResults[modelPath][f'{s}, {c}'][str(t)]\n",
    "            area = rectTriArea(frameData, diceList)\n",
    "            \n",
    "            topAreas.append((modelPath, s, c, t, area))\n",
    "\n",
    "topAreas = sorted(topAreas, key= lambda x : x[4], reverse= True)\n",
    "\n",
    "allModelNames = \"\"\n",
    "for modelName in topModelPaths:\n",
    "    allModelNames = allModelNames + modelName +'\\n'\n",
    "\n",
    "areasResult = {str(i): [] for i in range(len(topAreas)+1)}\n",
    "areasResult['0'].append(f'''Search results for: \\n \n",
    "                    {allModelNames} \n",
    "                    {len(scales)} Scales from [{scales[0]} {scales[-1]}\\n \n",
    "                    {len(centers)} Centers from [{centers[0]} {centers[-1]}\\n \n",
    "                    {len(thresholds)} Thresholds from [{thresholds[0]} {thresholds[-1]}\\n \n",
    "                    ''')\n",
    "\n",
    "for i in range(len(topAreas)):\n",
    "    areasResult[f'{i+1}'] = topAreas[i]\n",
    "\n",
    "currentTime = datetime.now()\n",
    "formatTime = currentTime.strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "areaParamPath = f'{parentResultsPath}\\Area_and_Params_{formatTime}.txt'\n",
    "\n",
    "with open(areaParamPath, 'w') as saveFile:\n",
    "    saveFile.write(json.dumps(areasResult))\n",
    "    \n",
    "#with open(areaParamPath, 'w') as saveFile:\n",
    "#    allModelNames = \"\"\n",
    "#    for modelName in topModelPaths:\n",
    "#        allModelNames = allModelNames + modelName +'\\n'\n",
    "#    infoString = f'''Search results for: \\n \n",
    "#                    {allModelNames} \n",
    "#                    {len(scales)} Scales from [{scales[0]} {scales[-1]}\\n \n",
    "#                    {len(centers)} Centers from [{centers[0]} {centers[-1]}\\n \n",
    "#                    {len(thresholds)} Thresholds from [{thresholds[0]} {thresholds[-1]}\\n \n",
    "#                    '''\n",
    "#    saveFile.write(infoString)\n",
    "#    for tup in topAreas:\n",
    "#        saveFile.write(str(tup) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    topAreas = globals()[topAreas]\n",
    "    topAreas = topAreas[10:]\n",
    "    print('Loading Top Areas from local variable...')\n",
    "except:\n",
    "    areasResultFilePath = r'' #your path here\n",
    "    print('Loading Top Areas from file path...')\n",
    "    with open(areasResultFilePath, 'r') as file:\n",
    "        areasResult = json.load(file)\n",
    "    topAreas = []\n",
    "    for i in range(1,len(areasResult)):\n",
    "        topAreas.append(areasResult[str(i)])\n",
    "\n",
    "try:\n",
    "    allDiceResults = globals()[allDiceResults]\n",
    "    print('Loading All Dice Results from local variable...')\n",
    "except:\n",
    "    allDiceFilePath = r'' #your path here\n",
    "    print('Loading All Dice Results from file path...')\n",
    "    with open(allDiceFilePath, 'r') as file:\n",
    "        allDiceResults = json.load(file)  \n",
    "\n",
    "try:\n",
    "    allMetricResults = globals()[allMetricResults]\n",
    "    print('Loading All Metrics Results from local variable...')\n",
    "except:\n",
    "    allMetricsFilePath = r'' #your path here\n",
    "    print('Loading All Metrics Results from file path...')\n",
    "    with open(allMetricsFilePath, 'r') as file:\n",
    "        allMetricResults = json.load(file)  \n",
    "\n",
    "\"\"\"\"\n",
    "if allFrameResults:\n",
    "    print('Loading All Frame Results from local variable...')\n",
    "else:\n",
    "    allFrameFilePath = '' #your path here\n",
    "    print('Loading All Frame Results from file path...')\n",
    "    allFrameResults = torch.load(allFrameFilePath)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xCoords = [n for n in range(1, 101)]\n",
    "n = 5\n",
    "currentTime = datetime.now()\n",
    "formatTime = currentTime.strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "topNResultPath = f'{parentResultsPath}\\\\Top_{n}_Dice_Graph{formatTime}.png'\n",
    "top1ResultPath = f'{parentResultsPath}\\\\Top_1_Dice_Graph{formatTime}.png'\n",
    "\n",
    "for i in range(n):\n",
    "    modelPath = topAreas[i][0]\n",
    "    modelScale = topAreas[i][1]\n",
    "    modelCenter = topAreas[i][2]\n",
    "    modelThresh = topAreas[i][3]\n",
    "    parts = modelPath.split(\"\\\\\")\n",
    "    modelName = parts[3]\n",
    "    yCoords = allDiceResults[modelPath][f'{modelScale}, {modelCenter}'][str(modelThresh)]\n",
    "    plt.plot(xCoords, yCoords, label = f'{modelName} | S: {modelScale} | C: {modelCenter} | T: {modelThresh}')\n",
    "\n",
    "plt.title(f\"Top {n} Results by Model Params\")\n",
    "plt.legend(fontsize = 'small', loc = 'upper left', bbox_to_anchor=(1,1))\n",
    "plt.savefig(topNResultPath)\n",
    "plt.show()\n",
    "\n",
    "xCoords = [n for n in range(1, 101)]\n",
    "n = 1\n",
    "\n",
    "for i in range(n):\n",
    "    modelPath = topAreas[i][0]\n",
    "    modelScale = topAreas[i][1]\n",
    "    modelCenter = topAreas[i][2]\n",
    "    modelThresh = topAreas[i][3]\n",
    "    parts = modelPath.split(\"\\\\\")\n",
    "    modelName = parts[3]\n",
    "    yCoords = allDiceResults[modelPath][f'{modelScale}, {modelCenter}'][str(modelThresh)]\n",
    "    plt.plot(xCoords, yCoords, label = f'{modelName} | S: {modelScale} | C: {modelCenter} | T: {modelThresh}')\n",
    "\n",
    "plt.title(f\"Top Result by Model Params\")\n",
    "plt.legend(fontsize = 'small', loc = 'upper left', bbox_to_anchor=(1,1))\n",
    "plt.savefig(top1ResultPath)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topModelParams = []\n",
    "\n",
    "topModelNum = 1\n",
    "\n",
    "#downscale = torchvision.transforms.Compose([\n",
    "#    torchvision.transforms.Resize((16, 64))])\n",
    "\n",
    "testImagePath = r\"\"\n",
    "testLabelPath = r\"\"\n",
    "\n",
    "logModelFitLoader= image_processor.imageDirsToTestLoader(imageDir=testImagePath, labelDir= testLabelPath)\n",
    "\n",
    "for i in range(topModelNum):\n",
    "    topModelParams.append([topAreas[i][0], topAreas[i][1], topAreas[i][2], topAreas[i][3]])\n",
    "\n",
    "segmentationPath = f\"{parentResultsPath}\\Segmentations\"\n",
    "\n",
    "if not os.path.exists(segmentationPath):\n",
    "    os.makedirs(segmentationPath)\n",
    "\n",
    "\n",
    "for modelName, s, c, t in topModelParams:\n",
    "    currentTime = datetime.now()\n",
    "    formatTime = currentTime.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "    print(f'START: {modelName} | {formatTime}')\n",
    "    model = unet.unet()\n",
    "    model.load_state_dict(torch.load(modelName))\n",
    "    #model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    for case, (image, label) in enumerate(logModelFitLoader):\n",
    "        #image = image.to(device)\n",
    "        #label = label.to(device)\n",
    "        \n",
    "        pred = model(image)\n",
    "\n",
    "        predSig = mapImageTensor(pred, center= c, scale= s)\n",
    "        \n",
    "        for frame in range(predSig.size(0)): #split each batch to single frames\n",
    "            frameImage = predSig[frame, :, :, :]\n",
    "            frameLabel = label[frame, :, :, :]\n",
    "            predThresh = torch.where(frameImage <= t, 0, 255)\n",
    "\n",
    "            framePlotter(predThresh, frameLabel, frame+1, case+1, modelName, segmentationPath)\n",
    "    \n",
    "\n",
    "    #for s, c in scaleCentersProduct:\n",
    "    #    currentTime = datetime.now()\n",
    "    #    formatTime = currentTime.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "    #    print(f'SCALE: {s} - CENTER: {c} | {formatTime}')\n",
    "        \n",
    "\n",
    "        #for case in range(len(allFrameResults[modelPath][f'{s}, {c}'][f'{t}'])): #for every case\n",
    "        #    for frame, (predMask, label) in enumerate(allFrameResults[modelPath][f'{s}, {c}'][f'{t}'][f'{case+1}']): #for every frame\n",
    "        #        framePlotter(predMask, label, frame+1, case+1, modelPath, segmentationPath)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
