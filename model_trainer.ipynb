{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import unet\n",
    "import image_processor\n",
    "import torch\n",
    "import os\n",
    "import zipfile\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def modelWriter(model, path : str):\n",
    "    torch.save(model.state_dict(), path)\n",
    "    \n",
    "#thanks chatGPT\n",
    "def zip_file(file_path, zip_path):\n",
    "    with zipfile.ZipFile(zip_path, 'w', zipfile.ZIP_DEFLATED) as zipf:\n",
    "        zipf.write(file_path, arcname=os.path.basename(file_path))\n",
    "    os.remove(file_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#imageDir = \"Data_Renamed\\processed_data\\\\test_data_downsampled_16x\\images_rename\"\n",
    "#labelDir = \"Data_Renamed\\processed_data\\\\test_data_downsampled_16x\\labels_rename\"\n",
    "imageDir = \"Data_Renamed\\\\raw_data\\\\train_val\\dataset_2\\images_rename\"\n",
    "labelDir = \"Data_Renamed\\\\raw_data\\\\train_val\\dataset_2\\labels_rename\"\n",
    "outputPath = \"Output_Models\"\n",
    "\n",
    "#labelTransform = torchvision.transforms.Compose([\n",
    "#    torchvision.transforms.Resize((16, 64))])\n",
    "\n",
    "name = \"unet_norm\"\n",
    "\n",
    "batchSizes = [16]\n",
    "epochs = [100]\n",
    "learningRates = [.001]\n",
    "labelDim = 262144\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = 'cuda'\n",
    "else:\n",
    "    device = 'cpu'\n",
    "\n",
    "parentOut = f'{outputPath}/{name}/weightsNorm/'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCHS: 100 | LR: 0.001 | BATCH: 16\n",
      "EPOCH: 1 | 2024-03-29 05:47:09\n",
      "EPOCH: 1 | LOSS: 237020.203125 | 2024-03-29 06:06:48\n",
      "unet_norm_VL_4771_E_100_B_16_LR_0.001 saved\n",
      "EPOCH: 2 | 2024-03-29 06:09:50\n",
      "EPOCH: 2 | LOSS: 204755.46875 | 2024-03-29 06:30:21\n",
      "unet_norm_VL_4914_E_100_B_16_LR_0.001 saved\n",
      "EPOCH: 3 | 2024-03-29 06:33:20\n",
      "EPOCH: 3 | LOSS: 200905.265625 | 2024-03-29 06:52:50\n",
      "unet_norm_VL_4713_E_100_B_16_LR_0.001 saved\n",
      "EPOCH: 4 | 2024-03-29 06:55:25\n",
      "EPOCH: 4 | LOSS: 199982.640625 | 2024-03-29 07:13:14\n",
      "unet_norm_VL_4786_E_100_B_16_LR_0.001 saved\n",
      "EPOCH: 5 | 2024-03-29 07:15:51\n",
      "EPOCH: 5 | LOSS: 205843.3125 | 2024-03-29 07:33:40\n",
      "unet_norm_VL_4586_E_100_B_16_LR_0.001 saved\n",
      "EPOCH: 6 | 2024-03-29 07:36:17\n",
      "EPOCH: 6 | LOSS: 16967018.0 | 2024-03-29 07:54:09\n",
      "unet_norm_VL_5014_E_100_B_16_LR_0.001 saved\n",
      "EPOCH: 7 | 2024-03-29 07:56:45\n",
      "EPOCH: 7 | LOSS: 1871205.125 | 2024-03-29 08:14:37\n",
      "unet_norm_VL_4858_E_100_B_16_LR_0.001 saved\n",
      "EPOCH: 8 | 2024-03-29 08:17:13\n",
      "EPOCH: 8 | LOSS: 341346.1875 | 2024-03-29 08:35:01\n",
      "unet_norm_VL_4763_E_100_B_16_LR_0.001 saved\n",
      "EPOCH: 9 | 2024-03-29 08:37:38\n",
      "EPOCH: 9 | LOSS: 645696.875 | 2024-03-29 08:55:28\n",
      "unet_norm_VL_4738_E_100_B_16_LR_0.001 saved\n",
      "EPOCH: 10 | 2024-03-29 08:58:12\n",
      "EPOCH: 10 | LOSS: 193126.09375 | 2024-03-29 09:21:08\n",
      "unet_norm_VL_4579_E_100_B_16_LR_0.001 saved\n",
      "EPOCH: 11 | 2024-03-29 09:24:46\n",
      "EPOCH: 11 | LOSS: 199058.34375 | 2024-03-29 09:42:35\n",
      "unet_norm_VL_4743_E_100_B_16_LR_0.001 saved\n",
      "EPOCH: 12 | 2024-03-29 09:45:12\n",
      "EPOCH: 12 | LOSS: 206884.359375 | 2024-03-29 10:03:52\n",
      "unet_norm_VL_4654_E_100_B_16_LR_0.001 saved\n",
      "EPOCH: 13 | 2024-03-29 10:06:28\n",
      "EPOCH: 13 | LOSS: 195991.359375 | 2024-03-29 10:24:13\n",
      "unet_norm_VL_4572_E_100_B_16_LR_0.001 saved\n",
      "EPOCH: 14 | 2024-03-29 10:26:49\n",
      "EPOCH: 14 | LOSS: 188975.734375 | 2024-03-29 10:44:35\n",
      "unet_norm_VL_4198_E_100_B_16_LR_0.001 saved\n",
      "EPOCH: 15 | 2024-03-29 10:47:11\n",
      "EPOCH: 15 | LOSS: 190918.875 | 2024-03-29 11:04:58\n",
      "unet_norm_VL_4613_E_100_B_16_LR_0.001 saved\n",
      "EPOCH: 16 | 2024-03-29 11:07:33\n",
      "EPOCH: 16 | LOSS: 193375.984375 | 2024-03-29 11:25:20\n",
      "unet_norm_VL_4832_E_100_B_16_LR_0.001 saved\n",
      "EPOCH: 17 | 2024-03-29 11:27:56\n",
      "EPOCH: 17 | LOSS: 190037.578125 | 2024-03-29 11:45:43\n",
      "unet_norm_VL_4652_E_100_B_16_LR_0.001 saved\n",
      "EPOCH: 18 | 2024-03-29 11:48:19\n",
      "EPOCH: 18 | LOSS: 178094.03125 | 2024-03-29 12:06:05\n",
      "unet_norm_VL_4338_E_100_B_16_LR_0.001 saved\n",
      "EPOCH: 19 | 2024-03-29 12:08:41\n",
      "EPOCH: 19 | LOSS: 187775.140625 | 2024-03-29 12:26:26\n",
      "unet_norm_VL_5090_E_100_B_16_LR_0.001 saved\n",
      "EPOCH: 20 | 2024-03-29 12:29:01\n",
      "EPOCH: 20 | LOSS: 238672.46875 | 2024-03-29 12:46:48\n",
      "unet_norm_VL_7775821_E_100_B_16_LR_0.001 saved\n",
      "EPOCH: 21 | 2024-03-29 12:49:24\n",
      "EPOCH: 21 | LOSS: 11941167104.0 | 2024-03-29 13:07:10\n",
      "unet_norm_VL_34381888_E_100_B_16_LR_0.001 saved\n",
      "EPOCH: 22 | 2024-03-29 13:09:45\n",
      "EPOCH: 22 | LOSS: 47919028.0 | 2024-03-29 13:27:31\n",
      "unet_norm_VL_4541_E_100_B_16_LR_0.001 saved\n",
      "EPOCH: 23 | 2024-03-29 13:30:07\n",
      "EPOCH: 23 | LOSS: 184993.890625 | 2024-03-29 13:47:52\n",
      "unet_norm_VL_4173_E_100_B_16_LR_0.001 saved\n",
      "EPOCH: 24 | 2024-03-29 13:50:33\n",
      "EPOCH: 24 | LOSS: 168466.328125 | 2024-03-29 14:08:18\n",
      "unet_norm_VL_3936_E_100_B_16_LR_0.001 saved\n",
      "EPOCH: 25 | 2024-03-29 14:10:54\n",
      "EPOCH: 25 | LOSS: 160978.96875 | 2024-03-29 14:28:41\n",
      "unet_norm_VL_3698_E_100_B_16_LR_0.001 saved\n",
      "EPOCH: 26 | 2024-03-29 14:31:16\n",
      "EPOCH: 26 | LOSS: 154783.875 | 2024-03-29 14:49:02\n",
      "unet_norm_VL_3668_E_100_B_16_LR_0.001 saved\n",
      "EPOCH: 27 | 2024-03-29 14:51:58\n",
      "EPOCH: 27 | LOSS: 153571.703125 | 2024-03-29 15:10:33\n",
      "unet_norm_VL_3650_E_100_B_16_LR_0.001 saved\n",
      "EPOCH: 28 | 2024-03-29 15:13:08\n",
      "EPOCH: 28 | LOSS: 152268.65625 | 2024-03-29 15:30:53\n",
      "unet_norm_VL_3484_E_100_B_16_LR_0.001 saved\n",
      "EPOCH: 29 | 2024-03-29 15:33:29\n",
      "EPOCH: 29 | LOSS: 145342.609375 | 2024-03-29 15:51:16\n",
      "unet_norm_VL_3512_E_100_B_16_LR_0.001 saved\n",
      "EPOCH: 30 | 2024-03-29 15:53:51\n",
      "EPOCH: 30 | LOSS: 144063.921875 | 2024-03-29 16:11:37\n",
      "unet_norm_VL_3463_E_100_B_16_LR_0.001 saved\n",
      "EPOCH: 31 | 2024-03-29 16:14:13\n",
      "EPOCH: 31 | LOSS: 141925.84375 | 2024-03-29 16:31:58\n",
      "unet_norm_VL_3314_E_100_B_16_LR_0.001 saved\n",
      "EPOCH: 32 | 2024-03-29 16:34:34\n",
      "EPOCH: 32 | LOSS: 139130.9375 | 2024-03-29 16:52:18\n",
      "unet_norm_VL_3338_E_100_B_16_LR_0.001 saved\n",
      "EPOCH: 33 | 2024-03-29 16:54:54\n",
      "EPOCH: 33 | LOSS: 137679.796875 | 2024-03-29 17:12:42\n",
      "unet_norm_VL_3269_E_100_B_16_LR_0.001 saved\n",
      "EPOCH: 34 | 2024-03-29 17:15:18\n",
      "EPOCH: 34 | LOSS: 133864.265625 | 2024-03-29 17:33:02\n",
      "unet_norm_VL_3446_E_100_B_16_LR_0.001 saved\n",
      "EPOCH: 35 | 2024-03-29 17:35:38\n",
      "EPOCH: 35 | LOSS: 131888.53125 | 2024-03-29 17:53:25\n",
      "unet_norm_VL_3134_E_100_B_16_LR_0.001 saved\n",
      "EPOCH: 36 | 2024-03-29 17:56:00\n",
      "EPOCH: 36 | LOSS: 136749.6875 | 2024-03-29 18:13:49\n",
      "unet_norm_VL_3157_E_100_B_16_LR_0.001 saved\n",
      "EPOCH: 37 | 2024-03-29 18:16:24\n",
      "EPOCH: 37 | LOSS: 128847.90625 | 2024-03-29 18:34:11\n",
      "unet_norm_VL_3132_E_100_B_16_LR_0.001 saved\n",
      "EPOCH: 38 | 2024-03-29 18:36:46\n",
      "EPOCH: 38 | LOSS: 129041.09375 | 2024-03-29 18:54:32\n",
      "unet_norm_VL_3074_E_100_B_16_LR_0.001 saved\n",
      "EPOCH: 39 | 2024-03-29 18:57:08\n",
      "EPOCH: 39 | LOSS: 126736.1640625 | 2024-03-29 19:15:54\n",
      "unet_norm_VL_3059_E_100_B_16_LR_0.001 saved\n",
      "EPOCH: 40 | 2024-03-29 19:18:28\n",
      "EPOCH: 40 | LOSS: 125486.1796875 | 2024-03-29 19:36:12\n",
      "unet_norm_VL_3049_E_100_B_16_LR_0.001 saved\n",
      "EPOCH: 41 | 2024-03-29 19:38:47\n",
      "EPOCH: 41 | LOSS: 123857.8046875 | 2024-03-29 19:56:31\n",
      "unet_norm_VL_2967_E_100_B_16_LR_0.001 saved\n",
      "EPOCH: 42 | 2024-03-29 19:59:07\n",
      "EPOCH: 42 | LOSS: 121450.2578125 | 2024-03-29 20:16:52\n",
      "unet_norm_VL_3092_E_100_B_16_LR_0.001 saved\n",
      "EPOCH: 43 | 2024-03-29 20:19:28\n",
      "EPOCH: 43 | LOSS: 121911.2421875 | 2024-03-29 20:38:03\n",
      "unet_norm_VL_2945_E_100_B_16_LR_0.001 saved\n",
      "EPOCH: 44 | 2024-03-29 20:40:38\n",
      "EPOCH: 44 | LOSS: 118737.0234375 | 2024-03-29 20:58:23\n",
      "unet_norm_VL_2880_E_100_B_16_LR_0.001 saved\n",
      "EPOCH: 45 | 2024-03-29 21:01:00\n",
      "EPOCH: 45 | LOSS: 114137.2890625 | 2024-03-29 21:18:47\n",
      "unet_norm_VL_2919_E_100_B_16_LR_0.001 saved\n",
      "EPOCH: 46 | 2024-03-29 21:21:23\n",
      "EPOCH: 46 | LOSS: 113811.828125 | 2024-03-29 21:39:08\n",
      "unet_norm_VL_2917_E_100_B_16_LR_0.001 saved\n",
      "EPOCH: 47 | 2024-03-29 21:41:45\n",
      "EPOCH: 47 | LOSS: 110521.359375 | 2024-03-29 22:00:19\n",
      "unet_norm_VL_2851_E_100_B_16_LR_0.001 saved\n",
      "EPOCH: 48 | 2024-03-29 22:02:59\n",
      "EPOCH: 48 | LOSS: 108513.984375 | 2024-03-29 22:20:43\n",
      "unet_norm_VL_2789_E_100_B_16_LR_0.001 saved\n",
      "EPOCH: 49 | 2024-03-29 22:23:19\n",
      "EPOCH: 49 | LOSS: 115993.1484375 | 2024-03-29 22:41:05\n",
      "unet_norm_VL_2818_E_100_B_16_LR_0.001 saved\n",
      "EPOCH: 50 | 2024-03-29 22:43:40\n",
      "EPOCH: 50 | LOSS: 105563.4140625 | 2024-03-29 23:01:25\n",
      "unet_norm_VL_2914_E_100_B_16_LR_0.001 saved\n",
      "EPOCH: 51 | 2024-03-29 23:04:00\n",
      "EPOCH: 51 | LOSS: 102446.0546875 | 2024-03-29 23:21:44\n",
      "unet_norm_VL_2689_E_100_B_16_LR_0.001 saved\n",
      "EPOCH: 52 | 2024-03-29 23:24:19\n",
      "EPOCH: 52 | LOSS: 101174.328125 | 2024-03-29 23:42:04\n",
      "unet_norm_VL_3183_E_100_B_16_LR_0.001 saved\n",
      "EPOCH: 53 | 2024-03-29 23:44:40\n",
      "EPOCH: 53 | LOSS: 109292.1875 | 2024-03-30 00:02:26\n",
      "unet_norm_VL_2753_E_100_B_16_LR_0.001 saved\n",
      "EPOCH: 54 | 2024-03-30 00:05:03\n",
      "EPOCH: 54 | LOSS: 100897.7890625 | 2024-03-30 00:22:48\n",
      "unet_norm_VL_2674_E_100_B_16_LR_0.001 saved\n",
      "EPOCH: 55 | 2024-03-30 00:25:25\n",
      "EPOCH: 55 | LOSS: 95350.2890625 | 2024-03-30 00:43:09\n",
      "unet_norm_VL_2702_E_100_B_16_LR_0.001 saved\n",
      "EPOCH: 56 | 2024-03-30 00:45:46\n",
      "EPOCH: 56 | LOSS: 92228.2890625 | 2024-03-30 01:03:33\n",
      "unet_norm_VL_2573_E_100_B_16_LR_0.001 saved\n",
      "EPOCH: 57 | 2024-03-30 01:06:08\n",
      "EPOCH: 57 | LOSS: 91041.8515625 | 2024-03-30 01:23:53\n",
      "unet_norm_VL_2561_E_100_B_16_LR_0.001 saved\n",
      "EPOCH: 58 | 2024-03-30 01:26:29\n",
      "EPOCH: 58 | LOSS: 87588.1953125 | 2024-03-30 01:44:14\n",
      "unet_norm_VL_2772_E_100_B_16_LR_0.001 saved\n",
      "EPOCH: 59 | 2024-03-30 01:46:50\n",
      "EPOCH: 59 | LOSS: 87123.8671875 | 2024-03-30 02:04:37\n",
      "unet_norm_VL_2587_E_100_B_16_LR_0.001 saved\n",
      "EPOCH: 60 | 2024-03-30 02:07:13\n",
      "EPOCH: 60 | LOSS: 83060.515625 | 2024-03-30 02:24:59\n",
      "unet_norm_VL_2744_E_100_B_16_LR_0.001 saved\n",
      "EPOCH: 61 | 2024-03-30 02:27:35\n",
      "EPOCH: 61 | LOSS: 84981.640625 | 2024-03-30 02:45:21\n",
      "unet_norm_VL_2606_E_100_B_16_LR_0.001 saved\n",
      "EPOCH: 62 | 2024-03-30 02:48:01\n",
      "EPOCH: 62 | LOSS: 80217.140625 | 2024-03-30 03:05:47\n",
      "unet_norm_VL_2400_E_100_B_16_LR_0.001 saved\n",
      "EPOCH: 63 | 2024-03-30 03:08:22\n",
      "EPOCH: 63 | LOSS: 76731.7421875 | 2024-03-30 03:26:08\n",
      "unet_norm_VL_2732_E_100_B_16_LR_0.001 saved\n",
      "EPOCH: 64 | 2024-03-30 03:28:45\n",
      "EPOCH: 64 | LOSS: 75271.1484375 | 2024-03-30 03:46:32\n",
      "unet_norm_VL_2388_E_100_B_16_LR_0.001 saved\n",
      "EPOCH: 65 | 2024-03-30 03:49:09\n",
      "EPOCH: 65 | LOSS: 76075.7578125 | 2024-03-30 04:06:56\n",
      "unet_norm_VL_2456_E_100_B_16_LR_0.001 saved\n",
      "EPOCH: 66 | 2024-03-30 04:09:32\n",
      "EPOCH: 66 | LOSS: 74127.96875 | 2024-03-30 04:27:17\n",
      "unet_norm_VL_2398_E_100_B_16_LR_0.001 saved\n",
      "EPOCH: 67 | 2024-03-30 04:29:52\n",
      "EPOCH: 67 | LOSS: 70429.421875 | 2024-03-30 04:47:39\n",
      "unet_norm_VL_2325_E_100_B_16_LR_0.001 saved\n",
      "EPOCH: 68 | 2024-03-30 04:50:15\n",
      "EPOCH: 68 | LOSS: 68986.2734375 | 2024-03-30 05:07:58\n",
      "unet_norm_VL_2260_E_100_B_16_LR_0.001 saved\n",
      "EPOCH: 69 | 2024-03-30 05:10:35\n",
      "EPOCH: 69 | LOSS: 66979.9765625 | 2024-03-30 05:28:20\n",
      "unet_norm_VL_2249_E_100_B_16_LR_0.001 saved\n",
      "EPOCH: 70 | 2024-03-30 05:30:56\n",
      "EPOCH: 70 | LOSS: 63557.19140625 | 2024-03-30 05:48:41\n",
      "unet_norm_VL_2292_E_100_B_16_LR_0.001 saved\n",
      "EPOCH: 71 | 2024-03-30 05:51:17\n",
      "EPOCH: 71 | LOSS: 68459.9921875 | 2024-03-30 06:09:02\n",
      "unet_norm_VL_2424_E_100_B_16_LR_0.001 saved\n",
      "EPOCH: 72 | 2024-03-30 06:11:37\n",
      "EPOCH: 72 | LOSS: 64810.390625 | 2024-03-30 06:29:23\n",
      "unet_norm_VL_2252_E_100_B_16_LR_0.001 saved\n",
      "EPOCH: 73 | 2024-03-30 06:31:59\n",
      "EPOCH: 73 | LOSS: 59688.5 | 2024-03-30 06:50:54\n",
      "unet_norm_VL_2153_E_100_B_16_LR_0.001 saved\n",
      "EPOCH: 74 | 2024-03-30 06:53:30\n",
      "EPOCH: 74 | LOSS: 58455.96484375 | 2024-03-30 07:11:15\n",
      "unet_norm_VL_2161_E_100_B_16_LR_0.001 saved\n",
      "EPOCH: 75 | 2024-03-30 07:13:50\n",
      "EPOCH: 75 | LOSS: 55316.94921875 | 2024-03-30 07:31:36\n",
      "unet_norm_VL_2076_E_100_B_16_LR_0.001 saved\n",
      "EPOCH: 76 | 2024-03-30 07:34:11\n",
      "EPOCH: 76 | LOSS: 54416.1015625 | 2024-03-30 07:51:57\n",
      "unet_norm_VL_2192_E_100_B_16_LR_0.001 saved\n",
      "EPOCH: 77 | 2024-03-30 07:54:33\n",
      "EPOCH: 77 | LOSS: 57183.23046875 | 2024-03-30 08:12:20\n",
      "unet_norm_VL_2057_E_100_B_16_LR_0.001 saved\n",
      "EPOCH: 78 | 2024-03-30 08:14:56\n",
      "EPOCH: 78 | LOSS: 57599.1171875 | 2024-03-30 08:32:44\n",
      "unet_norm_VL_2111_E_100_B_16_LR_0.001 saved\n",
      "EPOCH: 79 | 2024-03-30 08:35:19\n",
      "EPOCH: 79 | LOSS: 53732.85546875 | 2024-03-30 08:53:07\n",
      "unet_norm_VL_2238_E_100_B_16_LR_0.001 saved\n",
      "EPOCH: 80 | 2024-03-30 08:55:43\n",
      "EPOCH: 80 | LOSS: 51002.96484375 | 2024-03-30 09:13:27\n",
      "unet_norm_VL_2089_E_100_B_16_LR_0.001 saved\n",
      "EPOCH: 81 | 2024-03-30 09:16:06\n",
      "EPOCH: 81 | LOSS: 49445.69921875 | 2024-03-30 09:34:36\n",
      "unet_norm_VL_2084_E_100_B_16_LR_0.001 saved\n",
      "EPOCH: 82 | 2024-03-30 09:37:11\n",
      "EPOCH: 82 | LOSS: 49451.57421875 | 2024-03-30 09:54:56\n",
      "unet_norm_VL_2047_E_100_B_16_LR_0.001 saved\n",
      "EPOCH: 83 | 2024-03-30 09:57:31\n",
      "EPOCH: 83 | LOSS: 46631.30859375 | 2024-03-30 10:15:16\n",
      "unet_norm_VL_2183_E_100_B_16_LR_0.001 saved\n",
      "EPOCH: 84 | 2024-03-30 10:17:52\n",
      "EPOCH: 84 | LOSS: 48233.83984375 | 2024-03-30 10:35:37\n",
      "unet_norm_VL_2010_E_100_B_16_LR_0.001 saved\n",
      "EPOCH: 85 | 2024-03-30 10:38:13\n",
      "EPOCH: 85 | LOSS: 45830.76953125 | 2024-03-30 10:55:58\n",
      "unet_norm_VL_2363_E_100_B_16_LR_0.001 saved\n",
      "EPOCH: 86 | 2024-03-30 10:58:34\n",
      "EPOCH: 86 | LOSS: 45293.05078125 | 2024-03-30 11:16:19\n",
      "unet_norm_VL_2069_E_100_B_16_LR_0.001 saved\n",
      "EPOCH: 87 | 2024-03-30 11:18:55\n",
      "EPOCH: 87 | LOSS: 42926.359375 | 2024-03-30 11:36:41\n",
      "unet_norm_VL_2055_E_100_B_16_LR_0.001 saved\n",
      "EPOCH: 88 | 2024-03-30 11:39:18\n",
      "EPOCH: 88 | LOSS: 43876.0625 | 2024-03-30 11:57:02\n",
      "unet_norm_VL_2151_E_100_B_16_LR_0.001 saved\n",
      "EPOCH: 89 | 2024-03-30 11:59:39\n",
      "EPOCH: 89 | LOSS: 44765.6953125 | 2024-03-30 12:17:24\n",
      "unet_norm_VL_2064_E_100_B_16_LR_0.001 saved\n",
      "EPOCH: 90 | 2024-03-30 12:19:59\n",
      "EPOCH: 90 | LOSS: 41349.94921875 | 2024-03-30 12:37:45\n",
      "unet_norm_VL_2058_E_100_B_16_LR_0.001 saved\n",
      "EPOCH: 91 | 2024-03-30 12:40:20\n",
      "EPOCH: 91 | LOSS: 40032.5390625 | 2024-03-30 12:58:05\n",
      "unet_norm_VL_1880_E_100_B_16_LR_0.001 saved\n",
      "EPOCH: 92 | 2024-03-30 13:00:41\n",
      "EPOCH: 92 | LOSS: 38950.640625 | 2024-03-30 13:19:14\n",
      "unet_norm_VL_2227_E_100_B_16_LR_0.001 saved\n",
      "EPOCH: 93 | 2024-03-30 13:21:50\n",
      "EPOCH: 93 | LOSS: 40650.9375 | 2024-03-30 13:39:36\n",
      "unet_norm_VL_2001_E_100_B_16_LR_0.001 saved\n",
      "EPOCH: 94 | 2024-03-30 13:42:12\n",
      "EPOCH: 94 | LOSS: 38691.2578125 | 2024-03-30 14:00:11\n",
      "unet_norm_VL_1969_E_100_B_16_LR_0.001 saved\n",
      "EPOCH: 95 | 2024-03-30 14:03:13\n",
      "EPOCH: 95 | LOSS: 38682.28515625 | 2024-03-30 14:21:05\n",
      "unet_norm_VL_2160_E_100_B_16_LR_0.001 saved\n",
      "EPOCH: 96 | 2024-03-30 14:23:40\n",
      "EPOCH: 96 | LOSS: 38253.6796875 | 2024-03-30 14:41:27\n",
      "unet_norm_VL_1980_E_100_B_16_LR_0.001 saved\n",
      "EPOCH: 97 | 2024-03-30 14:44:18\n",
      "EPOCH: 97 | LOSS: 35269.359375 | 2024-03-30 15:02:45\n",
      "unet_norm_VL_1907_E_100_B_16_LR_0.001 saved\n",
      "EPOCH: 98 | 2024-03-30 15:05:55\n",
      "EPOCH: 98 | LOSS: 36256.7578125 | 2024-03-30 15:23:56\n",
      "unet_norm_VL_2031_E_100_B_16_LR_0.001 saved\n",
      "EPOCH: 99 | 2024-03-30 15:26:31\n",
      "EPOCH: 99 | LOSS: 35956.30078125 | 2024-03-30 15:44:59\n",
      "unet_norm_VL_1928_E_100_B_16_LR_0.001 saved\n",
      "EPOCH: 100 | 2024-03-30 15:47:47\n",
      "EPOCH: 100 | LOSS: 35802.01953125 | 2024-03-30 16:05:37\n",
      "unet_norm_VL_2029_E_100_B_16_LR_0.001 saved\n"
     ]
    }
   ],
   "source": [
    "for e in epochs:\n",
    "    for lr in learningRates:\n",
    "        for bs in batchSizes:\n",
    "            model = unet.unet(labelDim=labelDim)\n",
    "            model.to(device)\n",
    "            \n",
    "\n",
    "\n",
    "            trainLoader, valLoader = image_processor.imageDirsToLoaders(imageDir=imageDir, labelDir= labelDir, batchSize= bs)\n",
    "            optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "            loss = torch.nn.MSELoss()\n",
    "\n",
    "            print(f'EPOCHS: {e} | LR: {lr} | BATCH: {bs}')\n",
    "            epochLosses = []\n",
    "            validationLosses = []\n",
    "\n",
    "            for i in range(e):\n",
    "                model.train()\n",
    "                currentTime = datetime.now()\n",
    "                formatTime = currentTime.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "                print(f'EPOCH: {i+1} | {formatTime}')\n",
    "                runningLoss = 0\n",
    "                epochLoss = 0\n",
    "                for image, label in trainLoader:\n",
    "                    image = image.to(device)\n",
    "                    label = label.to(device)\n",
    "                    optimizer.zero_grad()\n",
    "                    outputClasses = model(image)\n",
    "                    \n",
    "                    #outputLogits = torch.sigmoid(outputClasses)\n",
    "                    #labelLogits = torch.sigmoid(label)\n",
    "                    batchLoss = loss(outputClasses, label)\n",
    "                    \n",
    "                    batchLoss.backward()\n",
    "                    optimizer.step()\n",
    "                    epochLoss += batchLoss\n",
    "\n",
    "                currentTime = datetime.now()\n",
    "                formatTime = currentTime.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "                print(f\"EPOCH: {i+1} | LOSS: {epochLoss} | {formatTime}\")\n",
    "\n",
    "                epochLosses.append(epochLoss)\n",
    "                batchLoss = 0\n",
    "                epochLoss = 0\n",
    "            \n",
    "                validationLoss = 0\n",
    "\n",
    "                model.eval()\n",
    "                with torch.no_grad():\n",
    "                    for image, label in valLoader:\n",
    "                        image = image.to(device)\n",
    "                        label = label.to(device)\n",
    "                        outputs = model(image)\n",
    "                        valBatchLoss = loss(outputs, label)\n",
    "                        validationLoss += valBatchLoss\n",
    "                \n",
    "                validationLoss = round(validationLoss.item()/len(valLoader))\n",
    "                validationLosses.append(validationLoss)\n",
    "                model.validationLoss = validationLosses\n",
    "                model.traningLosses = epochLosses\n",
    "\n",
    "                detailedName = name + \"_VL_\" + str(validationLoss) + \"_E_\" + str(e) + \"_B_\" + str(bs) + \"_LR_\" + str(lr)\n",
    "                if not os.path.exists(parentOut):\n",
    "                    os.makedirs(parentOut)\n",
    "                writeOut = os.path.join(parentOut, f'{detailedName}.pt')\n",
    "                modelWriter(model, writeOut)\n",
    "                print(f'{detailedName} saved')\n",
    "\n",
    "            \n",
    "            \n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
